{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d8ad712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1606eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open(\"C:\\Users\\dylan\\OneDrive\\Documents\\Dylans Doc's\\Dylan's courses\\2024 Courses\\IronHack\\Week 8\\Labs\\Lab 1\\gpt_key.txt\", \"r\").read().strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8cbb449",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77be79",
   "metadata": {},
   "source": [
    "# EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[ # messages parameter must be a list of dictionaries\n",
    "    # can be as short as one message or many back and forth turns.\n",
    "    {\"role\": \"system\", \"content\": \"You are a famous chef. Share your best cooking tips and tricks.\"}, # each dictionary has a role and content\n",
    "    {\"role\": \"user\", \"content\": \"What are the ingredients for making pancakes?\"},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0f5954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-9bYqEtPEAkRMksPl4bq6dhtVrJei6 at 0x24cb56d4470> JSON: {\n",
       "  \"id\": \"chatcmpl-9bYqEtPEAkRMksPl4bq6dhtVrJei6\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1718739094,\n",
       "  \"model\": \"gpt-3.5-turbo-0125\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"To make pancakes, you will need the following ingredients:\\n\\n- 1 cup all-purpose flour\\n- 2 tablespoons granulated sugar\\n- 1 teaspoon baking powder\\n- 1/2 teaspoon baking soda\\n- 1/4 teaspoon salt\\n- 1 cup milk\\n- 1 large egg\\n- 2 tablespoons melted butter or oil\\n- Optional: vanilla extract, cinnamon, or other flavorings\\n\\nMix the dry ingredients in one bowl and the wet ingredients in another. Then, combine them gently until just mixed - don't overmix, a few lumps are okay. Cook on a hot, greased skillet or griddle until bubbles form on the surface, then flip and cook until golden brown on both sides. Serve with your favorite toppings like maple syrup, fresh fruits, or whipped cream. Enjoy your delicious pancakes!\"\n",
       "      },\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 33,\n",
       "    \"completion_tokens\": 170,\n",
       "    \"total_tokens\": 203\n",
       "  },\n",
       "  \"system_fingerprint\": null\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5cc1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with text, and your task is to translate it into emojis. Do not use any regular text. Do your best with emojis only.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Artificial intelligence is a technology with great promise.\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.8,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bcd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"You will be provided with text, and your task is to translate it into emojis. Do not use any regular text. Do your best with emojis only.\"}]\n",
    "\n",
    "def gpt_response(inp, message_history):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=message_history\n",
    "    )\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ed1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ðŸ¤–\n",
      "ðŸ‘‹\n",
      "ðŸ‘‹ðŸ‘‹\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m gpt_response(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> \u001b[39m\u001b[38;5;124m\"\u001b[39m), message_history) \u001b[38;5;66;03m# Lets ask by input different prompts\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"), message_history) # Lets ask by input different prompts\n",
    "    print(message_history[-1][\"content\"]) # Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Gradio for Front-End UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history1 =[{\"role\": \"system\", \"content\": \"You will be provided with text, and your task is to translate it into emojis. Do not use any regular text. Do your best with emojis only.\"},\n",
    "                   {\"role\": \"assistant\", \"content\": f\"OK\"}]\n",
    "\n",
    "def gpt_response_ui(inp):\n",
    "    global message_history1\n",
    "    \n",
    "    message_history1 = gpt_response(inp, message_history1, max_tokens = 50)\n",
    "\n",
    "    #Get pairs (as tuples) of msg[\"content\"] from message history,representing one exchange between the user and the chatbot, skipping the pre-prompt\n",
    "    response = [(message_history1[i][\"content\"], message_history1[i+1][\"content\"]) for i in range(2, len(message_history1)-1, 2)]  # convert to tuples of list\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_response(inp, message_history, **params):\n",
    "    # We save the user's input\n",
    "    message_history1.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": message_history1,\n",
    "        **params  # Include additional parameters\n",
    "    }\n",
    "\n",
    "    completion = openai.ChatCompletion.create(**completion_params) # Include additional parameters\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history1.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo: #creates a Gradio interface\n",
    "\n",
    "    chatbot = gr.Chatbot() #creates a chatbot instance\n",
    "\n",
    "    with gr.Row(): #creates a row within the Gradio interface to contain components\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\") #text input field\n",
    "        # `show_label=False` parameter hides the label associated with the textbox\n",
    "\n",
    "    txt.submit(gpt_response_ui, txt, chatbot) #sets the submit action to the `gpt_response` function\n",
    "    txt.submit(lambda :\"\", None, txt) #this clears the textbox when the user submits their input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "Running on public URL: https://b584afd47c44af7760.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b584afd47c44af7760.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 532, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1514, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 832, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_52560\\3094815338.py\", line 7, in gpt_response_ui\n",
      "    message_history1 = gpt_response(inp, message_history1, max_tokens = 50)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_52560\\3667553250.py\", line 12, in gpt_response\n",
      "    completion = openai.ChatCompletion.create(**completion_params) # Include additional parameters\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-XZP2f***************************************USGu. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 532, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1514, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 832, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_52560\\3094815338.py\", line 7, in gpt_response_ui\n",
      "    message_history1 = gpt_response(inp, message_history1, max_tokens = 50)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_52560\\3667553250.py\", line 12, in gpt_response\n",
      "    completion = openai.ChatCompletion.create(**completion_params) # Include additional parameters\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\dylan\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-XZP2f***************************************USGu. You can find your API key at https://platform.openai.com/account/api-keys.\n"
     ]
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
